{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNcoOAudEQl4H6RUBpBPmAX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sukyeong02/24-1-AI/blob/main/11_3_RNN_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tjAZWzAF0CE",
        "outputId": "a307e939-16b3-43d6-b83d-52b2affb8a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hello': 1, 'hi there!': 2, 'how are you?': 3, \"i'm doing well, thanks.\": 4, \"what's your name?\": 5, \"i'm a chatbot.\": 6, \"what's your favorite song?\": 7, 'my favorite song is sudden shower!!': 8}\n",
            "[[1, 2], [3, 4], [5, 6], [7, 8]]\n",
            "Max Input Len:  2\n",
            "==================:  4\n",
            "[[1], [3], [5], [7]]\n",
            "[[2], [4], [6], [8]]\n",
            "Size:  9\n",
            "Epoch 1/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1953 - accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1897 - accuracy: 0.7500\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1840 - accuracy: 0.7500\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.1783 - accuracy: 0.7500\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1725 - accuracy: 1.0000\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1666 - accuracy: 1.0000\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1606 - accuracy: 1.0000\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1545 - accuracy: 1.0000\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1482 - accuracy: 1.0000\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 2.1417 - accuracy: 1.0000\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1351 - accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.1282 - accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1211 - accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1138 - accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.1061 - accuracy: 1.0000\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0982 - accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0899 - accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0813 - accuracy: 1.0000\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0723 - accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0630 - accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0532 - accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0430 - accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0323 - accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0212 - accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.0096 - accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.9974 - accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.9847 - accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.9714 - accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.9576 - accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.9431 - accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.9280 - accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.9121 - accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.8956 - accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.8784 - accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.8604 - accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.8417 - accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.8221 - accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.8017 - accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.7804 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.7583 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.7353 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.7113 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.6864 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6605 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.6337 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6058 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.5769 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.5471 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.5162 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.4843 - accuracy: 1.0000\n",
            "input seq:  [[3]]\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Predict Output:  [[[0.07524264 0.09960864 0.12503092 0.07853104 0.22745061 0.09212933\n",
            "   0.12203588 0.08556542 0.09440552]]]\n",
            "Predict Index:  [[4]]\n",
            "User: how are you?\n",
            "Chatbot: i'm doing well, thanks.\n",
            "input seq:  [[7]]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Predict Output:  [[[0.08949882 0.06873005 0.13352068 0.09065987 0.08903171 0.07358583\n",
            "   0.12542862 0.08476435 0.2447801 ]]]\n",
            "Predict Index:  [[8]]\n",
            "User: What's your favorite song?\n",
            "Chatbot: my favorite song is sudden shower!!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "conversations = [\n",
        "    [\"Hello\", \"Hi there!\"],\n",
        "    [\"How are you?\", \"I'm doing well, thanks.\"],\n",
        "    [\"What's your name?\", \"I'm a chatbot.\"],\n",
        "    [\"What's your favorite song?\", \"My favorite song is sudden shower!!\"]\n",
        " ]\n",
        "\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(conversations)\n",
        "print(token.word_index)\n",
        "\n",
        "sequences = token.texts_to_sequences(conversations)\n",
        "print(sequences)\n",
        "\n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "print(\"Max Input Len: \", max_sequence_len)\n",
        "\n",
        "print(\"==================: \", len(sequences))\n",
        "X = []; y = []\n",
        "for idx in range(len(sequences)):\n",
        "   X.append([sequences[idx][0]])\n",
        "   y.append([sequences[idx][1]])\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "word_size = len(token.word_index) + 1\n",
        "print(\"Size: \", word_size)\n",
        "\n",
        "model = Sequential()\n",
        "model.add( Embedding(word_size, 64, input_length=max_sequence_len, mask_zero=True) )\n",
        "model.add( LSTM(100, return_sequences=True) )\n",
        "model.add( Dense(word_size, activation='softmax') )\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X, y, epochs=50, verbose=1)\n",
        "\n",
        "def generate_response(input_text):\n",
        "    input_seq = token.texts_to_sequences([[input_text]])\n",
        "    print(\"input seq: \", input_seq)\n",
        "    predicted_output = model.predict(input_seq)\n",
        "    print(\"Predict Output: \", predicted_output)\n",
        "\n",
        "    predicted_word_index = tf.argmax(predicted_output, axis=-1).numpy()\n",
        "    print(\"Predict Index: \", predicted_word_index)\n",
        "    response = token.sequences_to_texts(predicted_word_index)\n",
        "    return response[0]\n",
        "\n",
        "user_input = \"how are you?\"\n",
        "response = generate_response(user_input)\n",
        "print(f\"User: {user_input}\")\n",
        "print(f\"Chatbot: {response}\")\n",
        "\n",
        "user_input = \"What's your favorite song?\"\n",
        "response = generate_response(user_input)\n",
        "print(f\"User: {user_input}\")\n",
        "print(f\"Chatbot: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yn0TjL3OGOy8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}